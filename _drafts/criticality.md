the goal is not to show my understanding, the goal is to give the other person understanding. empathy has been lacking in this direction

title:

The case is that which is the world

description:

Water and ice are two forms, two <i>phases</i> of what is fundamentally and essentially the same substance. Thermodynamics is the physical science which, among other things, explains these physical changes in what is essentially the same matter. What happens if you apply the tools of thermodynamics to virtual substance, to computation?

to post immediately after main post:

there are a lot of pdfs in this fpp, I may have missed marking some. Some are scientific journal papers: I tried to get arXiv papers when I could, but I missed some.
my background is also nearly all in computer science and cognitive science in this domain, so my physics is probably lacking.

things I left out:
chialvo, prigogne, thoroughgoing critique of Bak, Bak in general

main post:

That matter changes in form and properties but not in fundamental nature when it becomes a certain temperature was part of the subject of the first debates on the nature of what exists. It was inspiration to the ancients, including Plato, Leucippus, Epicurus and Democritus, who had his name most famously attached to the ancient atomism. By saying that there were indivisible units of matter, they opposed the assertion of Parmenides, that there was no essential change in the world, that the apparent change of things was mere illusion, because the fundamental particles could combine to make up the matter that makes up the world.

The ancient atomists were right in the essential fact. The world is made out of elementary particles. However, their sole usage of pure argumentation in claiming this thesis meant that they couldn't really make great predictions and shore up the million details of how materials work in practice. A discussion. http://plato.stanford.edu/entries/atomism-ancient/ A history. http://www.math.uga.edu/~davide/Phase_Transitions_and_Critical_Phenomena.pdf (pdf)

The modern understanding of phase transition has included another phase of matter (plasma http://pluto.space.swri.edu/image/glossary/plasma.html) and entirely other phenomena, such as the emergence of superconductivity in certain materials when cooled (http://www.supraconductivite.fr/en/index.php?p=supra-explication-phase), the superfluidity emerging of liquid helium (https://www.youtube.com/watch?v=2Z6UJbwxBZI), and the emergence of ferromagnetism in ferromagnetic material(http://marie.ph.surrey.ac.uk/~phs1rs/teaching/ising.pdf).

Now, the phenomenon I mentioned above the fold is kind of tangential to the FPP itself. This is because there are two kinds of phase transitions: ones that involve a latent heat (http://en.wikipedia.org/wiki/Latent_heat), called first-order phase transitions(which would include the water-ice transition), and ones which are continuous. These continuous (second-order) phase transitions involve, among other interesting properties, a diverging correlation length which leads to so-called critical phenomena, which correspond to critical points in a phase diagram like this one http://www.innovateus.net/science/what-critical-point .

In critical phenomena, the divergence of correlation length means that microscopic details of the system are washed out, and that means that critical phenomena can be categorized into one of a few universality classes. Meaning, the phase transitions themselves can be categorized depending on the way in which they diverge, using only a quantity called the critical exponent (http://physics.stackexchange.com/questions/80245/first-and-second-order-phase-transitions).

What does that mean in plain-ish English? In two paragraphs, a precis of the really quite nice popularization here (http://physics.ohio-state.edu/~jay/846/Wilson.pdf):

In nature, there are a great diversity of lengths of scales, from the mote upon the wind to the grandeur of the oceans. In most times and in most theories we think of events which are greatly different in size as having little to nothing to do with each other: we don't need to think about the molecular structure of liquid water when we note the mark of the tides.

But there are phenomena, like water at its "critical point", where events at very many of the scales matter, all at once. Fluctuations in density of water occurr at all possible scales, all the way to infinity, and if you are to have a theory of the density of water at its critical point, you must take account of both a single molecule to an infinite amount of water.

Now, it actually turns out that universality, as the systematic sameness of the properties of divergence which only depend upon the critical exponent in thermodynamic systems is called, is more universal than simply in thermodynamic systems. There's a lot of <i>dynamical</i> systems where you get to a scaling limit, when a large number of parts come together in a dynamical system, there comes around some interesting properties. From systems as abstract as iterated maps, Mitchell Feigenbaum discovered, you can still get universality. I think this encyclopedia article is a very friendly development of it as long as you're not afraid of math (http://www.encyclopediaofmath.org/index.php/Universal_behaviour_in_dynamical_systems).

Also closely related is the phenomenon of symmetry-breaking, where small fluctuations in a system crossing a critical point determine the system's fate. It is a starting-point for one of the really <i>scientific</i> criticisms of reductionism, by PW Anderson ("More is Different"). I was highly amused by his epigraphic quote. http://robotics.cs.tamu.edu/dshell/cs689/papers/anderson72more_is_different.pdf

There's this mathematical tool called the renormalization group. It's a way to look at physical systems viewed at different distance scales systematically. The inspiration is strange: fractals. The paper of Mandelbrot(http://en.wikipedia.org/wiki/How_Long_Is_the_Coast_of_Britain%3F_Statistical_Self-Similarity_and_Fractional_Dimension) that introduced the idea of self-similarity in distance measurement (althought not the term, <i>fractal</i>) to the world dealt with a fundamentally similar problem, which was easier to spot. If you measure a coastline of a landmass with a big ruler, you get a short distance. As you make the ruler smaller, you get a longer and longer distance as things go on. You can't have a final length of the coastline, but you <i>can</i> measure how <i>fast</i> the coastline's length gets bigger as you make the rulers smaller. So that was what Mandelbrot measured.

Note that this makes statistical mechanics the first and probably most successful complex systems science, in that it totally <i>can</i> explain emergent behavior by recourse to scientific reduction. One could make the case that the exploration of fractals and the simulation methodology which people used is not reductionist, perhaps, though, but nearly all of these scientists held to the reductionist program. Here's an interesting article-like thing on the issue. http://vserver1.cscs.lsa.umich.edu/~crshalizi/notabene/emergent-properties.html

Let's now talk about the computers, because we want to. "What does that have to do with the previous section of this FPP?" you ask.

=== entropy, information, Shannon information

Let's talk about neural networks. Something about them was biologically inspired, but from the first, cyberneticists, dynamicists, statisticians and thermodynamicists have been poking at them, wondering at what kind of simple mathematical principles you could run with, in order to simulate well in a scale-free manner the human neuron. Most people use some variation of backpropagation, which has no real meaning vis-a-vis biology and is really a creature of calculus, a clever way to use a gradient descent method. But a thermodynamic system was very important at a few points in the history of neurally-inspired coputing.

===computation: hopfield net, boltzmann machine. talk about hofstadter, talk about rbms/harmonium, stacked rbm, also by Hinton being the great big breakthrough in deep nets, but Rumelhart ditched the biological inspiration. hardware inspiration of vlsi, talk about Perceptrons and a little history of neural nets, random Feynman with Connection machine article

Many people, in present experience, fall in love with computation because of the deep beauty of the fractal geometry of nature. For much the same reason, they don't study it in depth. It is well known that Mandelbrot was the originator of the first ideas of the fractal theory, and also interesting to hear of his financial and economic analysis (his cotton prices paper http://web.williams.edu/Mathematics/sjmiller/public_html/341Fa09/econ/Mandelbroit_VariationCertainSpeculativePrices.pdf (pdf), which is deeply technical), although it's friendlier put by NN Taleb(http://www.fooledbyrandomness.com/), who acknowledges the influence.

One direction I want to direct your attention to, however, is the roughly-stated inverse fractal problem. You know that fractals are generated by elegant recursive programs of some sort (usually, to simplify the problem, one thinks of affine transformations). The inverse problem goes like this: given a fractal, how do you find the recursive program that generates the fractal (Mandelbrot's old home page goes on with extremely simple examples: http://classes.yale.edu/fractals/introtofrac/invprob/InvProb.html)? Obviously, this problem is related to statistical inference and machine learning.

This was a fairly well-treated problem in the 90's. The two main reasons why this isn't a well-treated problem in the 2010's is first that the developers of the fractal compression algorithms patented them and tried to make a company with it, which turned out to be a failure because of the second reason, which is that JPEG works pretty much as well in 1/1000 the time.

This is a fairly good treatment of the fundamental ideas used for a compression task. http://www.i-programmer.info/babbages-bag/482-fractal-image-compression.html . However, lossy compression is very related to machine learning and statistical inference, because they both create models of the data which exclude unnecessary detail while leaving essential detail out. https://hips.seas.harvard.edu/blog/2013/02/21/data-compression-and-unsupervised-learning/

Here's something which may be slightly interconnected. Let's move onto the more general question of what the nature of the shape of data is. Apparently, real data in structured places ends up being fractal a lot of the time. faloutsos, http://db.uwaterloo.ca/db_seminars/notes/christos.pdf

It's worth comparing this statement to the statement by the deep neural net people (actually, the machine learning people) about the problem of representation. Basically, the trivial representation of a picture to use in machine learning is to look at each pixel as a sliding scale from 0 to 255 or 65536 or whatever. That's a terrible idea because even a computer can't search in that 600,000-dimensional or whatever space. But actual data for actual things that you actually normally see is in a low-dimensional manifold in that space.

So you use a deep multilayer perceptron (or one of a bevy of other tools, all of them basically connectionist or Bayesian network-based) in order to create low-dimensional embeddings in that space. This is an advance because previously we were doing that shit by hand, or using single-hidden-layer MLPs. It's an interesting thing to think about, because it's important feature of manifolds that they are differentiable, and an important feature of fractals that they're not, and the current optimization methods for these spaces depend heavily on differentiation. I asked someone who actually knows math about this, and they conjectured that the deep learning kids don't really look into the manifold hypothesis too hard.

You get the same sort of "ooh, hey, edge detectors, or Gabor filters automatically pop up" in the (extremely old) fractal compression papers as the newer deep neural net papers. It may have to do with the renormalization group analysis for deep learning (and we know how renormalization group analysis relates to fractals, right?) with this hilariously-titled magazine article: https://www.quantamagazine.org/20141204-a-common-logic-to-seeing-cats-and-cosmos/ . It may have to do with the simple fact that natural images of interest simply universally have edges of some kind and that the compression of images must take advantage of this fact.

Let's talk about monkeys now. Specifically, abstract monkeys in the mind of George Miller, the great psychologist (he was the one who was "persecuted by an integer", the "Magic Number Seve, Plus Or Minus Two" http://www.musanim.com/miller1956/) who are typing all day (not necessarily Shakespeare, just random letters). At each time step, they type a space with some probability or a letter in the alphabet, with uniform probability from all of the letters of the alphabet. Despite the simplicity and stupidness of that model, you get a phenomena from it which is remarkable. The frequency rank of the "words" generated by this procedure acts like the frequency ranks in actual words in actual corpora: welcome to Zipf's law.

Actually that was sort of a lie. When you create a mathematical graph, out of stringing the nodes (which correspond to words) and edges (which correspond to word adjacencies) together from a large corpus, that statistics of this graph, the degree distribution, the clustering coefficients, the triad participations, they don't work well. It turns out what <i>does</i> work well is to use the formalisms that social network theorists have created in deriving the statistical structure of the Internet. There's the Statistical Kronecker graph, the Chung-Lu graph, the Multiplicative Attribute graph, there's even a Random Typing graph (it's a more sophisticated version of the abstract monkey model I mentioned, also turned into a graph). There's a Multifractal graph. I won't bore you with the menagerie, but the same network properties are characteristic of both real language and social networks(cancho and sole). It reminds me of Minsky's Society of Mind, myself, and of Nick Humprey's paper on the evolution of intelligence via sociality (http://cogprints.org/2694/1/SocialFunctionTxt.pdf).

Also, you remember PW Anderson's epigraph from "More is Different"? Another power law distribution, which applies to wealth, is the Pareto distribution, and it is the corresponding probability distribution to Zipf's law.

So, order parameters are an observable quantity that can distinguish between two phases of a physical system. In one phase, the order parameter is zero, in another phase it's non-zero. Stated abstractly like that, it ends up being really quite fun to look at <i>computational</i> systems in this way. Like this: https://theory.org/complexity/cdpt/html/node5.html . Or like this. http://www.lsv.ens-cachan.fr/Events/fmt2012/SLIDES/moshevardi.pdf http://ijcai.org/Past%20Proceedings/IJCAI-91-VOL1/PDF/052.pdf

Imagine a journey across a landscape where you cannot see across a significant distance: you only have an insignificantly-sized torch to get you where to go, and the landscape is in complete darkness otherwise. Your destination is the highest hill in the land. One of the ways to solve this abstract problem is to move along the steepest slope at each point, but the landscape is complex and it is dark, so just the fact that you are at the peak of a hill (a local optimum) does not mean that you are at the peak of the <i>highest<i> hill. Moreover, you, the human being is envisioning it in 3-dimensional space, but these problems must be solved in very high-dimensional spaces. The question of how to solve such global, nonconvex search problems is a very important one in the field of optimization.
=== talk about genetic algorithms and simulated annealing here, talk about order parameters HERE and some specific problems

Let's talk about something completely different.
=== THEN talk about scale-free search in animals (foraging, levy flights). what is scale-freedom? talk about it in relation to Gaussians, talk about it in relation to physical law

Here's a random algorithm closely inspired by actualy thermodynamics. Of course you're a metallurgist, right? Simulated annealing is a way to find good optima for extremely hard combinatorial problems, like the Travelling Salesman problem or what-have-you. It's like local search, with two tricks. The first is to randomly accept non-optimal solutions using something called the Metropolis algorithm, in order to <i>escape</i> local optima. The second trick is to tune a parameter in the Metropolis algorithm so that you escape <i>fewer</i> local optima over time, because you assume you've explored the global space more after escaping local optima really hard at the beginning. This is how misplaced atoms in a metal act: they sometimes get to even more random places, but as a metal cools, they tend to progress towards an equilibrium state. Tuning this algorithm is a dark art, but it gets you results.

Another domain where thermodynamic ideas are important is evolutionary biology. Some people bark up against thermodynamics in order to try to attack evolution, but it's fairly clear that's bollocks http://www.talkorigins.org/faqs/thermo.html . Some back-of-the-envelope calculations about evolution http://physics.gmu.edu/~roerter/EvolutionEntropy.htm . However, that is not important in this discussion, per se.

There is an important thing to say about evolutionary phenomena which Darwin did not predict and could not have predicted with the state of paleontological science in his day. That is, we know that the time dynamics of evolution is ruled by a sort of punctuated equilibrium, from Eldredge and Gould (http://evolution.berkeley.edu/evosite/evo101/VIIA1bPunctuated.shtml , http://en.wikipedia.org/wiki/Punctuated_equilibrium). Compare to the saying, "Natura non facit saltus" (Nature does not make jumps), which Mandelbrot and Taleb were both quite offended by, apparently (http://www.fooledbyrandomness.com/mandelbrotandhudson.pdf). This is a observation which is plagued by the multiplicity of possible causes, but so is much of paleontology, since it cannot be an experimental science. R Dawkins, for example, ever the friendly adversary of SJ Gould, said it wasn't really that important, and that the apparent gaps might have been caused by migratory events, and Dennett had a bunch of criticisms which Gould replied to here and here.
=== http://cogweb.ucla.edu/Debate/Gould.html, talk about the Dennett's thought

Where criticality would come in is in one of the ways the physicists ran with the idea of punctuated equilibrium. This was the Bak-Sneppen model of evolution, which is basically the simplest model which one could possibly make which still has the punctuated equilibrium dynamics http://www.jmu.edu/geology/evolutionarysystems/programs/bak-sneppendesc.pdf .  Also the subject, apparently, of one of the greatest bits of snark I've ever seen in academia (http://arxiv.org/pdf/adap-org/9910002v1.pdf) (check the acknowledgements)

Some other people also made an optimization algorithm out of it, which looks suspiciously like local search until you get to the scale-free dynamics. http://arxiv.org/abs/cond-mat/0010337 . It's not an evolutionary algorithm, in the technical sense of that term, because it's not population-based and so you can't do any fiddling with simulations of crossover and mutation. I will leave this link to explain what is done in the algorithm. http://www.cleveralgorithms.com/nature-inspired/physical/extremal_optimization.html

Let's end with a quote from J von Neumann, which I got from a Boltzmann machine paper (http://www.enterrasolutions.com/media/docs/2013/08/cogscibm.pdf):

"All of this will lead to theories [of computation] which are much less rigidly of an all-or-none nature than past and present formal logic. They will be of a much less combinatorial, and much more analytical, character. In fact, there are numerous indications to make us believe that this new system of formal logic will move closer to another disciplin which has been little linked in the past with logic. This is thermodynamics, primarily in the form it was received from Boltzmann, and is that part of theoretical physics which comes nearest in some of its aspects to manipulating and measuring information."
