---
layout: page
title: Poking at CNN
---

Neural networks are nonlinear iterated function systems. Actually, they're multiple nonlinear iterated function systems, one for the actual projections and activations, one for the bacpropagation of gradients, and so on (there is probably some deranged way you can think of them as a whole big nonlinear IFS). The nonlinearity in practice tends to be the most linear nonlinearity you can get away with. But its being a nonlinear IFS and an informative system immediately indicates that fractality should be looked-for. I tend to believe that this, not the fact that the activations are on any supposedly-smooth manifold, is what's important in getting neural networks to model informational phenomena, because information is equal to the sum of positive Lyapunov exponents - and positive Lyapunov exponents only, no probability need be involved.

Of course, there are many fractal phenomena that go on with neural networks. But it is noted that the pace of the fractal phenomena - the explosion of goodness with depth - seems to be contingent upon a sort of locality or something that I like to think of as a sort of lattice dimension. That is, in configuration space (and in actual simulation done on real computers) an Ising model is very high-dimensional, but the lattice that you're imagining the Ising model on is comparatively low-dimensional. This is very evocative of the rate of the space-filling in n-dimensional Hilbert curves and suchlike space-filling curves, which come with a nice analogue to the configuration space that both multilayer perceptron and Ising model both obviously have.

The immediate poking about is to model a CNN-like network but with elementwise multiplication on Hadamard transform (what is called dyadic convolution). Then, try to do local phenomena (convolution) and less local phenomena (dyadic convolution) with that sense of locality on the n-dimensional lattice.
